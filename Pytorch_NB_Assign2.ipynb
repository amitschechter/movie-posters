{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = \n",
    "NUM_TOTALIMAGES = \n",
    "DataSetFolder = \n",
    "BATCHSIZE =  #cifar10 set to 64\n",
    "R_Mean = \n",
    "G_Mean = \n",
    "B_Mean = \n",
    "R_Std = \n",
    "G_Std = \n",
    "B_Std = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is drafted after Assignment2 Pytorch - but those images were already square!!!!\n",
    "\n",
    "## to normalize by subtracting the mean RGB vlaue and dividing by the standard deviations of each RGB value\n",
    "## the Mean and STD are from CIFAR10 dataset\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((R_Mean, G_Mean, B_Mean), (R_std, G_Std, B_Std))])\n",
    "\n",
    "poster_train = dset.POSTERS(DataSetFolder, train=True, download =True,\n",
    "                             transform=transform)\n",
    "\n",
    "loader_train = DataLoader(poster_train, batch_size=BATCHSIZE, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "poster_val = dset.POSTERS(DataSetFolder, train=True, download=True, transform=transform)\n",
    "\n",
    "loader_val = DataLoader(poster_val, batch_size=BATCHSIZE, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, NUM_TOTALIMAGES)))\n",
    "\n",
    "poster_test = dset.POSTERS(DataSetFolder, train=False, download=True, transform=transform)\n",
    "\n",
    "loader_test = DataLoader(poster_test, batch_size=BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 \n",
    "\n",
    "if USE_GPU and torch.cuda.is_available(): \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] #read in N, C, H, W\n",
    "    return x.view(N, -1) # \"flatten\" the C * H * W values into a sing le vector per image\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2) \n",
    "    print('Before flattening: ', x) \n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): \n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model): \n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set') \n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # set model to evaluation mode \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype) # move to device, e.\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples , 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part(model, optimizer, epochs=1): \n",
    "    \"\"\"\n",
    "     - model: A PyTorch Module giving the model to train.\n",
    "     - optimizer: An Optimizer object we will use to train the model\n",
    "     - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device) # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train() # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype) # move to device, e.\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            \n",
    "            # Zero out all of the gradients for the variables which th\n",
    "            # will update.\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Actually update the parameters of the model using the gr\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch: %s' % (e))\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item())) \n",
    "                check_accuracy(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "channel_3 = 32\n",
    "channel_4 = 16\n",
    "learning_rate = 1e-2\n",
    "in_channel = 3\n",
    "num_classes = 10\n",
    "num_features = 3*32*32\n",
    "\n",
    "model = nn.Sequential(\n",
    "#Layer 1\n",
    "    nn.Conv2d(in_channel, channel_1, 5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(channel_1),\n",
    "#Layer2\n",
    "    nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(channel_2),\n",
    "    #Layer2.2 ====> added in as an experiment\n",
    "    nn.Conv2d(channel_2, channel_3, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(channel_3),\n",
    "#Layer3\n",
    "    nn.Conv2d(channel_3, channel_4, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(channel_4),\n",
    "#Layer4\n",
    "    Flatten(),\n",
    "    nn.Linear((channel_4*32*32), num_classes)\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "train_function(model, optimizer, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
