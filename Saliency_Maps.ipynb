{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "from image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "from torch.utils.data import DataLoader, sampler, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "class MOVIES(Dataset):\n",
    "\n",
    "    def __init__(self, images_folder, labels_doc, transform=None, preload=False, train=True):\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.filenames = []\n",
    "        self.root = images_folder\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "            \n",
    "        all_filenames = pd.read_csv(images_folder + '/' + labels_doc)        \n",
    "        \n",
    "        filenames = all_filenames['filename']\n",
    "        \n",
    "        # labels are read in this order:\n",
    "        # 'Action', 'Thriller', 'Adventure', 'Animation', 'Western', 'Comedy', \n",
    "        # 'Crime', 'Drama', 'Horror', 'Romance', 'Science Fiction', 'Fantasy', \n",
    "        # 'Family', 'Documentary', 'History', 'Music', 'Mystery', 'TV Movie', 'War'\n",
    "        labels = all_filenames[all_filenames.columns[2:]]\n",
    "        \n",
    "        for idx, fn in enumerate(filenames):\n",
    "            self.filenames.append((fn, list(labels.iloc[idx]))) # (filename, labels)\n",
    "\n",
    "        self.len = len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image_fn, label = self.filenames[index]\n",
    "        label = label[7] # Get drama label\n",
    "        image = Image.open('Data/posters_images/'+image_fn)\n",
    "\n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # return image and label\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Total number of samples in the dataset\n",
    "        return self.len\n",
    "    \n",
    "def load_data(batchsize=1):\n",
    "    image_folder = 'Data/poster_label_files'\n",
    "    Train_images_labels = 'Dataset_Training_3345.csv'\n",
    "    Val_images_labels = 'Dataset_Validation_955.csv'\n",
    "    Test_images_labels = 'Dataset_Test_479.csv'\n",
    "\n",
    "    BATCHSIZE = batchsize #cifar10 set to 64\n",
    "    \n",
    "    data_transforms = T.Compose([\n",
    "                T.Resize((224,224)),\n",
    "                T.ToTensor()])    \n",
    "\n",
    "    poster_train = MOVIES(image_folder, Train_images_labels, transform=data_transforms, train=True)\n",
    "    poster_val = MOVIES(image_folder, Val_images_labels, transform=data_transforms, train=True)\n",
    "    poster_test = MOVIES(image_folder, Test_images_labels, transform=data_transforms, train=True)\n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = DataLoader(poster_train, batch_size=BATCHSIZE)\n",
    "    dataloaders['val'] = DataLoader(poster_val, batch_size=BATCHSIZE)\n",
    "    dataloaders['test'] = DataLoader(poster_test, batch_size=BATCHSIZE)\n",
    "    \n",
    "    return dataloaders, poster_train, poster_val, poster_test\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size=224):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "                    std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img, should_rescale=True):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n",
    "        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n",
    "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "    \n",
    "def blur_image(X, sigma=1):\n",
    "    X_np = X.cpu().clone().numpy()\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the pretrained SqueezeNet model.\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in model_conv.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "\n",
    "# We don't want to train the model, so tell PyTorch not to compute gradients\n",
    "# with respect to model parameters.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# you may see warning regarding initialization deprecated, that's fine, please continue to next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=10\n",
    "dataloaders, poster_train, poster_val, poster_test = load_data(batchsize=batchsize)\n",
    "dataset_sizes = {}\n",
    "dataset_sizes['train'] = len(dataloaders['train'])\n",
    "dataset_sizes['val'] = len(dataloaders['val'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "i =0\n",
    "for images, labels in dataloaders['train']:\n",
    "    i =i +1\n",
    "    if i>5:\n",
    "        break\n",
    "    npimg = images[0].data.cpu().numpy()\n",
    "    npimg =np.transpose(npimg, (1, 2, 0))\n",
    "    X.append(npimg)\n",
    "    y.append(labels[0])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_example():\n",
    "    N, C = 4, 5\n",
    "    s = torch.randn(N, C)\n",
    "    y = torch.LongTensor([1, 2, 1, 3])\n",
    "    print(s)\n",
    "    print(y)\n",
    "    print(s.gather(1, y.view(-1, 1)).squeeze())\n",
    "gather_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    saliency = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image. You first want to compute the loss over the correct   #\n",
    "    # scores (we'll combine losses across a batch by summing), and then compute  #\n",
    "    # the gradients with a backward pass.                                        #\n",
    "    ##############################################################################\n",
    "    loss = model(X).gather(1, y.view(-1, 1)).squeeze().sum()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        saliency = torch.max(X.grad.abs(), dim =1)[0]\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_saliency_maps(X, y):\n",
    "    # Convert X and y from numpy arrays to Torch Tensors\n",
    "\n",
    "\n",
    "    X_tensor = torch.cat([preprocess(Image.fromarray(x.astype('uint8'))) for x in X], dim=0)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "\n",
    "    # Compute saliency maps for images in X\n",
    "    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
    "\n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    saliency = saliency.numpy()\n",
    "    N = X.shape[0]\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_saliency_maps(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
